TODO: 3-way parameters
  - find some simple task to test this with! it has to be VERY simple because the number of parameters is cubic.

TODO: 3-way factored parameters

TODO: gaussian units (sigma=1)

TODO: truncated exponential units

TODO: wat glue code om RBMs te stacken in DBNs.
    - samplen van een DBN
    - DBN training (laag-per-laag)
    - DBN inference

TODO: passen autoencoders ook binnen dit raamwerk? mss moeten bepaalde dingen dan hernoemd worden...

TODO: docs: what are the different components and their roles, what are vmaps, umaps, dmaps, ..., and what are the interfaces the different components adhere to (f.e. the 'cd' keyword argument for samplers)
+ some common use cases + example usage for each feature (f.e. how to use momentum, how to construct a CD updater)

TODO: cleanup: remove some comments that are irrelevant

TODO: optimised 1D convolution: visible_maps naar filter height, data (time) naar filter width
    
TODO: test 1D convolution

TODO: dat FCRBM model voor modeling motion style eens proberen nabouwen!
  ook interessant hierbij: parameter tying! kan dit in het framework gepast worden?
  IOTRBM en FIOTRBM zijn ook interessante modellen om eens na te bouwen.
  
TODO: gaussian units met learnt variance (gescheiden en samen)
  
TODO: symmetric beta units
    - instead of creating separate 'betaparameters', it might be a better idea to create some kind of wrapper for Units instances that functions as 1 - u (or just any transformation). Then a symmetric beta rbm can be implemented with the existing ProdParameters.

TODO: ReLU units

TODO: in a conditional RBM (f.e. with visibles v, hiddens h and context x), CD won't work if the context variable isn't supplied. This is because CD will try to update the parameters tying h/v and x regardless of whether x was supplied as context. It would be nice if this didn't happen, i.e. when x is not supplied as context, all parameters tying x are simply ignored. This would also avoid some hard-to-decipher error messages.

TODO: Fast persistent CD (FPCD) (Tieleman) - this will require another stats function that also keeps internal state (the fast weights)

TODO: supervised finetuning - how does this fit in the framework (separate trainers and updaters?)

TODO: Look into hessian-free optimisation, maybe that fits in as well? (although I think this is pretty much always supervised?)

TODO: ParamUpdaters compute additive updates for the parameters. But what if a more exotic training algorithm is used that does not use additive updates (multiplicative updates are a possibility)? Maybe it's better to let the ParamUpdater perform the addition as well?

TODO: add docstrings everywhere
 
TODO: maybe Tempered MCMC http://www.iro.umontreal.ca/~lisa/pointeurs/tempered_tech_report2009.pdf

TODO: docs: explain the need for supplying an 'initial vmap' (for stats, but also for training)
  + convention: minibatch contributions are SUMMED rather than averaged.

TODO: free energy (where it can be computed)

TODO: reconstruction cost is worthless for PCD, so if we're going to have PCD we'll also need something like pseudo-likelihood. Also adapt test_mnist_persistent.py to use this new measure. This requires free energy to be implemented.

TODO: spike and slab RBMs?


- This might also be interesting to have a look at, it has a PCD implementation (and also some other stuff like AIS) in Python + cuv:
https://github.com/deeplearningais/CUV/tree/master/examples/rbm







- Interesting train of thought: consider an RBM being trained for classification. Typically, it is pretrained unsupervisedly first, and then finetuned supervisedly. It is, however, also possible to combine both gradients and do both at once. This is suggested as future work in 'Training RBMs using approximations to the likelihood gradient' (PCD paper), so find out if they have done this yet.
When both gradients are combined, it would make sense to put the emphasis on the unsupervised gradient at the start, and more on the supervised gradient at the end. That means the weights of the respective gradients (in practice: the scale factors of the ParamUpdaters) vary during training.
Another use case where this would happen is when a variable (f.e. linearly decaying) learning rate is used.
To support these use cases, it has to be possible to use shared Theano variables as scale factors for ParamUpdaters. (The learning rate is precisely such a scale factor.) This makes it possible to update their values every epoch, or even every training step.
In fact, generally, it would be cool if any 'metaparameter' could be symbolic.
Theoretically this should already be possible, but it's a good idea to actually try some of these things and see if anything is still missing to support this.



*** PRELIMINARY DOCUMENTATION ***

TRAINING

split into multiple phases:

- collect statistics, given input data and the model
  * for CD-k, this is input visibles, sampled hiddens, and then visibles and hiddens after k steps
  * for PCD, the negative term is different
- use statistics to compute the weight updates
  * for all types of CD, this is just getting the 'gradient' from the Parameters object and filling in the blanks.
- update the weights according to some other hyperparameterised settings (this is where momentum, weight decay etc. are)

ParamUpdater: composite object (possibly consisting of multiple other ParamUpdaters that are somehow combined (typical usecase: updates are summed)) that updates parameters given some input data

DecayParamUpdater: provides a decay term

MomentumParamUpdater: encapsulates another ParamUpdater and applies momentum to it

CDParamUpdater: encapsulates CD-k or PCD weight update computation
  * takes the input data, computes statistics (Stats object)
  * gets the form of the update term from the Parameters object, fills in the blanks with the stats
  * returns the resulting update
  
SparsityParamUpdater: encapsulates sparsity target regularisation

SumParamUpdater: sums the updates obtained from its child paramupdaters. should check that the composing ParamUpdaters are for the same Parameters!

ScaleParamUpdater: takes a ParamUpdater and a scale factor, and scales the updates by this factor. This will be the result of writing '0.005 * ParamUpdater()' for example.


Stats: object that encapsulates statistics (typically for CD)

cd_stats: statistics for CD-k

pcd_stats: statistics for PCD

!! since only the negative term differs between cd_stats and pcd_stats, maybe some overlapping code can be factored out here.


MODULAR RBM

v = a(f(W, h) + g(vbias))
h = a'(f'(W, v) + g'(hbias))

ActivationFunction
Sampler
Units

(unit data u + activation functions a(x) + samplers) = Units

Parameters(list<Units> units)
  has
    - list<Units>: list of units that are related by the parameters
    - a set of actual parameters (weights)
  provides
    - contribution to activation of each of the Units
    - contribution to the energy function

RBM
  has
    - list<Units>: a set of different types of units
    - list<Parameters>: a set of different types of parameters that relate the Units
  provides
    - sampling a type of units (computing nonlinear activation and sampling)
    - computing the energy function (summing the contributions for each of the Parameters)


get unit values (this goes for all unit types):
  - compute linear activation
      x = sum_i(f(W_i, u_i))
  - apply activation function  
      a(x)
  - sample
      u ~ a(x)
      
Computing a linear activation consists of summing all the contributions of the different Parameters

a Sampler can just be the identity function (mean field) or a bernoulli sampler (most common), a softmax sampler, a max-pooling sampler, ...


Units, ActivationFunction and Sampler are AWARE of the RBM they are part of - most of the logic is concentrated in the subclasses. while this does bring some dependencies that could technically be avoided, it should lead to a cleaner architecture.


implement a system where different types of units, weights and sampling can be combined easily. If it's performant that's a plus, but it doesn't have to be (mainly for experimentation). This will make it easier to construct heterogeneous models.

